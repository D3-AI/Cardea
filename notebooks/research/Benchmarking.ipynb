{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle \n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from featuretools.selection import remove_low_information_features\n",
    "from cardea.benchmark import benchmark, aggregate_results_by_pipeline, aggregate_results_by_problem, CLASSIFICATION_METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path of the dataset files. Should be removed when the S3 source is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_dir = 'path/to/FeatureMatrices'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines\n",
    "\n",
    "pipelines = {\n",
    "#     'Logistic Regression': ['sklearn.preprocessing.MinMaxScaler', 'sklearn.linear_model.LogisticRegression'],\n",
    "    'K-Nearest Neightbors': ['sklearn.preprocessing.MinMaxScaler', 'sklearn.neighbors.KNeighborsClassifier'],\n",
    "    'Random Forest': ['sklearn.preprocessing.MinMaxScaler', 'sklearn.ensemble.RandomForestClassifier'],\n",
    "#     'Gaussian Naive Bayes': ['sklearn.preprocessing.MinMaxScaler', 'sklearn.naive_bayes.GaussianNB'],\n",
    "    'Multinomial Naive Bayes': ['sklearn.preprocessing.MinMaxScaler', 'sklearn.naive_bayes.MultinomialNB'],\n",
    "    'XGB': ['sklearn.preprocessing.MinMaxScaler', 'xgboost.XGBClassifier'],\n",
    "#     'Stochastic Gradient Descent': ['sklearn.preprocessing.MinMaxScaler', 'sklearn.linear_model.SGDClassifier'],\n",
    "    'Gradient Boosting': ['sklearn.preprocessing.MinMaxScaler', 'sklearn.ensemble.GradientBoostingClassifier']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "\n",
    "def load_feature_matrix(path):\n",
    "    df = pd.read_csv(path)\n",
    "    y = df.pop('label')\n",
    "    X = remove_low_information_features(df)\n",
    "    \n",
    "    col_num = len(X.columns)\n",
    "    X = X.fillna(0)\n",
    "    X = pd.get_dummies(X)\n",
    "    print(\"#features before one-hot-encoding: {}, #features after one-hot-encoding: {}\".format(col_num, len(X.columns)))\n",
    "    return X.join(y)\n",
    "\n",
    "def load_feature_tool_feature_matrix(problem):\n",
    "    path = os.path.join(fm_dir, \"fm_ft\", \"{}.csv\".format(problem))\n",
    "    return load_feature_matrix(path)\n",
    "\n",
    "def load_mimix_extract_feature_matrix(problem):\n",
    "    path = os.path.join(fm_dir, \"fm_me\", \"{}.csv\".format(problem))\n",
    "    return load_feature_matrix(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIMIC-Extract Datasets + Cardea AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = ['los', 'mortality', 'readmission']\n",
    "datasets = {p: load_mimix_extract_feature_matrix(p) for p in problems}\n",
    "    \n",
    "# sample small datasets for quick testing\n",
    "sample_datasets = {k: v.sample(n=1000, random_state=1) for k, v in datasets.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details in Each Execution of Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = benchmark(pipelines, sample_datasets, sample_datasets.keys(), target_name='label', \n",
    "                    optimize=False, runs=1, from_fm=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Each Pipeline in Each Problem-solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_summary = aggregate_results_by_pipeline(results, 'F1 Macro')\n",
    "pipeline_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Each Problem-solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_summary = aggregate_results_by_problem(results, 'F1 Macro')\n",
    "problem_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureTool + Cardea AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = ['los', 'mortality', 'readmission']\n",
    "datasets = {p: load_feature_tool_feature_matrix(p) for p in problems}\n",
    "    \n",
    "# sample small datasets for quick testing\n",
    "sample_datasets = {k: v.sample(n=1000, random_state=1) for k, v in datasets.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature numbers after one-hot-encoding are too large!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cardea-venv)",
   "language": "python",
   "name": "cardea-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
