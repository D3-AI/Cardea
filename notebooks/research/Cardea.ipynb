{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook contains the pipeline run of the analysis in cardea version 0.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from cardea import Cardea\n",
    "from cardea.modeling.modeler import Modeler\n",
    "from cardea.featurization import Featurization\n",
    "from cardea.data_loader.load_mimic import load_mimic_data\n",
    "\n",
    "from featuretools.selection import remove_low_information_features\n",
    "from model_audit import ModelAuditor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 1: Kaggle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd = Cardea()\n",
    "\n",
    "cd.load_data_entityset()\n",
    "cutoff = cd.select_problem('MissedAppointmentProblemDefinition')\n",
    "\n",
    "feature_matrix = cd.generate_features(cutoff)\n",
    "feature_matrix = feature_matrix.sample(frac=1)\n",
    "\n",
    "y = list(feature_matrix.pop('label'))\n",
    "X = feature_matrix.values\n",
    "\n",
    "pipelines = [\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'sklearn.naive_bayes.MultinomialNB']],\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'sklearn.ensemble.RandomForestClassifier']],\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'xgboost.XGBClassifier']],\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'sklearn.neighbors.KNeighborsClassifier']],\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'sklearn.linear_model.LogisticRegression']],\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'sklearn.linear_model.SGDClassifier']],\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'sklearn.ensemble.GradientBoostingClassifier']],\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'sklearn.naive_bayes.GaussianNB']]\n",
    "]\n",
    "\n",
    "problem_type = 'classification'\n",
    "\n",
    "for pipeline in pipelines:\n",
    "    hyperparameters = []\n",
    "    modeler = Modeler()\n",
    "\n",
    "    pipeline_res = cd.execute_pipeline(np.array(X), np.array(y), pipeline, problem_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 3: Adaptivity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines\n",
    "\n",
    "pipelines = [\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'sklearn.naive_bayes.MultinomialNB']],\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'sklearn.ensemble.RandomForestClassifier']],\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'xgboost.XGBClassifier']],\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'sklearn.neighbors.KNeighborsClassifier']],\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'sklearn.linear_model.LogisticRegression']],\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'sklearn.linear_model.SGDClassifier']],\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'sklearn.ensemble.GradientBoostingClassifier']],\n",
    "    [['sklearn.preprocessing.MinMaxScaler', 'sklearn.naive_bayes.GaussianNB']]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature(Featurization):\n",
    "    @staticmethod\n",
    "    def agg_prim():\n",
    "        return [\"sum\", \"std\", \"mode\", \"mean\", \"count\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def trans_prim():\n",
    "        return [\"day\", \"month\", \"year\", \"weekday\", \"is_weekend\"]\n",
    "\n",
    "    \n",
    "problems = ['los', 'mortality', 'readmission']\n",
    "for problem in problems:\n",
    "    es = load_mimic_data(path='MIMIC/') # data location\n",
    "    \n",
    "    if prob == 'mortality':\n",
    "        label_column = 'hospital_expire_flag'\n",
    "        time_column = 'admittime'\n",
    "        secondary_time_column = 'dischtime'\n",
    "        column_id = 'hadm_id'\n",
    "        entity = 'admissions'\n",
    "        remove_columns = ['deathtime', 'discharge_location', 'hospital_expire_flag']\n",
    "\n",
    "        entity_set_df = es[entity].df\n",
    "        es = es.entity_from_dataframe(entity_id=entity,\n",
    "                                      dataframe=entity_set_df,\n",
    "                                      time_index=time_column,\n",
    "                                      index=column_id,\n",
    "                                      secondary_time_index={secondary_time_column: remove_columns})\n",
    "\n",
    "    elif prob == 'readmission':\n",
    "        problem = 'readmission'\n",
    "        label_column = 'readmission'\n",
    "        time_column = 'dischtime'\n",
    "        column_id = 'hadm_id'\n",
    "        entity = 'admissions'\n",
    "\n",
    "    elif prob == 'los':\n",
    "        problem = 'los'\n",
    "        label_column = 'los'\n",
    "        time_column = 'intime'\n",
    "        column_id = 'icustay_id'\n",
    "        entity = 'icustays'\n",
    "        secondary_time_column = 'outtime'\n",
    "        remove_columns = ['last_wardid', 'last_careunit', 'los']\n",
    "\n",
    "        entity_set_df = es[entity].df\n",
    "        es = es.entity_from_dataframe(entity_id=entity,\n",
    "                                      dataframe=entity_set_df,\n",
    "                                      time_index=time_column,\n",
    "                                      index=column_id,\n",
    "                                      secondary_time_index={secondary_time_column: remove_columns})\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"problem not found.\")\n",
    "\n",
    "    cutoff_times = es[entity].df[[column_id, time_column, label_column]]\n",
    "    cutoff_times.columns = ['instance_id', 'time', 'label']\n",
    "    cutoff_times['time'] = cutoff_times['time']\n",
    "    cutoff_times = cutoff_times.sort_values('time')\n",
    "\n",
    "    feat = Feature()\n",
    "    fm_encoded, features_encoded = feat.generate_feature_matrix(es, entity, cutoff_times)\n",
    "\n",
    "    df = fm_encoded.copy()\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    try:\n",
    "        df = df.drop([problem], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    y = df.pop('label')\n",
    "    X = remove_low_information_features(df)\n",
    "    \n",
    "    X = X.fillna(0)\n",
    "    X = pd.get_dummies(X)\n",
    "\n",
    "    if problem == 'los':\n",
    "        y = np.digitize(y, [y.min(), 7, y.max()+1])\n",
    "    \n",
    "    y = pd.Categorical(y).codes\n",
    "    \n",
    "    problem_type = 'classification'\n",
    "    scoring_function = 'f1'\n",
    "    minimize_cost = False\n",
    "\n",
    "    for pipeline in pipelines:\n",
    "        hyperparameters = []\n",
    "        modeler = Modeler()\n",
    "\n",
    "        pipeline_res = modeler.execute_pipeline(np.array(X), np.array(y), pipeline, problem_type, optimize=False,\n",
    "                                                minimize_cost=minimize_cost, scoring=scoring_function, max_evals=10)\n",
    "\n",
    "        for k in pipeline_res.keys():\n",
    "            hyperparameters.append(pipeline_res[k]['hyperparameter'])\n",
    "\n",
    "        auditor = ModelAuditor()\n",
    "\n",
    "        def extract_metric(folds,metric_name):\n",
    "            metric_output = []\n",
    "            for fold in folds:\n",
    "                metric_output.append(fold[metric_name])\n",
    "            return metric_output\n",
    "\n",
    "        performance_result = []\n",
    "        for primitive, hyper in zip(pipeline, hyperparameters):\n",
    "            report_with_hyper = auditor.generate_pipeline_report(primitive, X, y,\n",
    "                                                                 problem_type, hyperparameters=hyper)\n",
    "            \n",
    "            print('completed tuning performance for {}'.format((primitive[-1])))\n",
    "            report_no_hyper = auditor.generate_pipeline_report(primitive, X, y, problem_type)\n",
    "            print('completed non-tuned performance for {}'.format(primitive[-1]))\n",
    "            print('===============================')\n",
    "\n",
    "            if problem_type == 'regression':\n",
    "                #r2_score and mse\n",
    "\n",
    "                # get non-tuned\n",
    "                result_arr = []\n",
    "                result_arr.append('no_hyper')\n",
    "                result_arr.append(primitive)\n",
    "                metrics = {}\n",
    "                metrics['r2_score'] = np.mean(extract_metric(report_no_hyper['output_result'],'r2_score'))\n",
    "                metrics['mean_squared_error'] = np.mean(extract_metric(report_no_hyper['output_result'],\n",
    "                                                                       'mean_squared_error'))\n",
    "                result_arr.append(metrics)\n",
    "                performance_result.append(result_arr)\n",
    "\n",
    "                # get tuned\n",
    "                result_arr = []\n",
    "                result_arr.append('with_hyper')\n",
    "                result_arr.append(primitive)\n",
    "                metrics = {}\n",
    "                metrics['r2_score'] = np.mean(extract_metric(report_with_hyper['output_result'],'r2_score'))\n",
    "                metrics['mean_squared_error'] = np.mean(extract_metric(report_with_hyper['output_result'],\n",
    "                                                                       'mean_squared_error'))\n",
    "                result_arr.append(metrics)\n",
    "                performance_result.append(result_arr)\n",
    "\n",
    "            elif problem_type == 'classification':\n",
    "                #f1_macro and accuracy\n",
    "\n",
    "                # get non-tuned\n",
    "                result_arr = []\n",
    "                result_arr.append('no_hyper')\n",
    "                result_arr.append(primitive)\n",
    "                metrics = {}\n",
    "                metrics['f1_macro'] = extract_metric(report_no_hyper['output_result'],'f1_macro')\n",
    "                metrics['accuracy'] = extract_metric(report_no_hyper['output_result'],'accuracy')\n",
    "                result_arr.append(metrics)\n",
    "                performance_result.append(result_arr)\n",
    "\n",
    "                # get tuned\n",
    "                result_arr = []\n",
    "                result_arr.append('with_hyper')\n",
    "                result_arr.append(primitive)\n",
    "                metrics = {}\n",
    "                metrics['f1_macro'] = extract_metric(report_with_hyper['output_result'],'f1_macro')\n",
    "                metrics['accuracy'] = extract_metric(report_with_hyper['output_result'],'accuracy')\n",
    "                result_arr.append(metrics)\n",
    "                performance_result.append(result_arr)\n",
    "\n",
    "\n",
    "        print(performance_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mimic_extract data\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('all_featurized_data.pkl', 'rb') as f:\n",
    "    all_datasets = pickle.load(f)\n",
    "    \n",
    "for problem, df in all_datasets.items():\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    y = np.array(df.pop('TARGET'))\n",
    "    X = df\n",
    "\n",
    "    for pipeline in pipelines:\n",
    "        hyperparameters = []\n",
    "        modeler = Modeler()\n",
    "\n",
    "        pipeline_res = modeler.execute_pipeline(np.array(X), np.array(y), pipeline, problem_type, optimize=False,\n",
    "                                                minimize_cost=minimize_cost, scoring=scoring_function, max_evals=10)\n",
    "\n",
    "        for k in pipeline_res.keys():\n",
    "            hyperparameters.append(pipeline_res[k]['hyperparameter'])\n",
    "\n",
    "        auditor = ModelAuditor()\n",
    "\n",
    "        def extract_metric(folds,metric_name):\n",
    "            metric_output = []\n",
    "            for fold in folds:\n",
    "                metric_output.append(fold[metric_name])\n",
    "            return metric_output\n",
    "\n",
    "        performance_result = []\n",
    "        for primitive, hyper in zip(pipeline, hyperparameters):\n",
    "            report_with_hyper = auditor.generate_pipeline_report(primitive, X, y,\n",
    "                                                                 problem_type, hyperparameters=hyper)\n",
    "            \n",
    "            print('completed tuning performance for {}'.format((primitive[-1])))\n",
    "            report_no_hyper = auditor.generate_pipeline_report(primitive, X, y, problem_type)\n",
    "            print('completed non-tuned performance for {}'.format(primitive[-1]))\n",
    "            print('===============================')\n",
    "\n",
    "            if problem_type == 'regression':\n",
    "                #r2_score and mse\n",
    "\n",
    "                # get non-tuned\n",
    "                result_arr = []\n",
    "                result_arr.append('no_hyper')\n",
    "                result_arr.append(primitive)\n",
    "                metrics = {}\n",
    "                metrics['r2_score'] = np.mean(extract_metric(report_no_hyper['output_result'],'r2_score'))\n",
    "                metrics['mean_squared_error'] = np.mean(extract_metric(report_no_hyper['output_result'],\n",
    "                                                                       'mean_squared_error'))\n",
    "                result_arr.append(metrics)\n",
    "                performance_result.append(result_arr)\n",
    "\n",
    "                # get tuned\n",
    "                result_arr = []\n",
    "                result_arr.append('with_hyper')\n",
    "                result_arr.append(primitive)\n",
    "                metrics = {}\n",
    "                metrics['r2_score'] = np.mean(extract_metric(report_with_hyper['output_result'],'r2_score'))\n",
    "                metrics['mean_squared_error'] = np.mean(extract_metric(report_with_hyper['output_result'],\n",
    "                                                                       'mean_squared_error'))\n",
    "                result_arr.append(metrics)\n",
    "                performance_result.append(result_arr)\n",
    "\n",
    "            elif problem_type == 'classification':\n",
    "                #f1_macro and accuracy\n",
    "\n",
    "                # get non-tuned\n",
    "                result_arr = []\n",
    "                result_arr.append('no_hyper')\n",
    "                result_arr.append(primitive)\n",
    "                metrics = {}\n",
    "                metrics['f1_macro'] = extract_metric(report_no_hyper['output_result'],'f1_macro')\n",
    "                metrics['accuracy'] = extract_metric(report_no_hyper['output_result'],'accuracy')\n",
    "                result_arr.append(metrics)\n",
    "                performance_result.append(result_arr)\n",
    "\n",
    "                # get tuned\n",
    "                result_arr = []\n",
    "                result_arr.append('with_hyper')\n",
    "                result_arr.append(primitive)\n",
    "                metrics = {}\n",
    "                metrics['f1_macro'] = extract_metric(report_with_hyper['output_result'],'f1_macro')\n",
    "                metrics['accuracy'] = extract_metric(report_with_hyper['output_result'],'accuracy')\n",
    "                result_arr.append(metrics)\n",
    "                performance_result.append(result_arr)\n",
    "\n",
    "\n",
    "        print(performance_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# los example correlation\n",
    "extract = pd.read_csv('features_merge_with_rowid.csv')\n",
    "cardea = [pd.read_csv('fm/los/fm%s.csv' % i) for i in range(0, 50)]\n",
    "cardea = pd.concat(cardea).set_index('hadm_id', drop=True)\n",
    "\n",
    "# match cohort\n",
    "mimic_extract = extract[extract['ROW_ID'].isin(cardea['row_id'])].drop_duplicates(['ROW_ID']).fillna(0)\n",
    "mimic_cardea = cardea[cardea['row_id'].isin(extract['ROW_ID'])].fillna(0)\n",
    "\n",
    "# sort\n",
    "mimic_extract.sort_values('ROW_ID', inplace=True)\n",
    "mimic_cardea.sort_values('row_id', inplace=True)\n",
    "\n",
    "# show consistency\n",
    "print('Extract', mimic_extract.shape)\n",
    "print('Cardea', mimic_cardea.shape)\n",
    "\n",
    "# convert to binary\n",
    "mimic_extract = pd.get_dummies(mimic_extract)\n",
    "mimic_cardea = pd.get_dummies(mimic_cardea)\n",
    "\n",
    "all_corr = []\n",
    "all_labels = []\n",
    "\n",
    "for col in tqdm(mimic_extract.columns):\n",
    "    corr = []\n",
    "    labels = []\n",
    "    for car in mimic_cardea.columns:\n",
    "        x = np.array(mimic_extract[col])\n",
    "        y = np.array(mimic_cardea[car])\n",
    "        \n",
    "        try:\n",
    "            pear = pearsonr(x, y)[0]\n",
    "            if not math.isnan(pear):\n",
    "                corr.append(pear)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    if len(corr) > 0:\n",
    "        idx = np.nanargmax(np.absolute(corr))\n",
    "        all_corr.append(corr[idx])\n",
    "        \n",
    "np.median(np.absolute(all_corr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
